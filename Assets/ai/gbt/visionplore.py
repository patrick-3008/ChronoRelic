# visionplore.py

import os
import json
import numpy as np
from datetime import datetime
from typing import List, Dict, Any, Optional
import openai
import chromadb
from chromadb.utils import embedding_functions
from chromadb.api.types import EmbeddingFunction, Documents, Images
import uuid
import pandas as pd
from PIL import Image
import torch
import torchvision.models as models
import torchvision.transforms as transforms
import mss
from dotenv import load_dotenv
import traceback
from collections import Counter # <-- Added this import

# --- Class Definition: OpenAIEmbeddingFunction ---
class OpenAIEmbeddingFunction(embedding_functions.EmbeddingFunction):
    """Custom embedding function using OpenAI's text-embedding models."""
    def __init__(self, api_key: str, model_name: str = "text-embedding-ada-002"):
        if not api_key:
            raise ValueError("OpenAI API key must be provided.")
        self.client = openai.OpenAI(api_key=api_key)
        self.model_name = model_name

    def __call__(self, input: Documents) -> embedding_functions.Embeddings:
        response = self.client.embeddings.create(input=input, model=self.model_name)
        return [data.embedding for data in response.data]

# --- Class Definition: ResNet50EmbeddingFunction ---
class ResNet50EmbeddingFunction(EmbeddingFunction):
    """Custom embedding function for images using a pre-trained ResNet50 model."""
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ResNet50 is running on device: {self.device}")
        self.model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT).to(self.device)
        self.model = torch.nn.Sequential(*(list(self.model.children())[:-1]))
        self.model.eval()
        self.preprocess = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])

    def __call__(self, input: Images) -> embedding_functions.Embeddings:
        embeddings = []
        for image_input in input:
            try:
                if isinstance(image_input, str):
                    if not os.path.exists(image_input):
                        raise FileNotFoundError(f"Image file not found: {image_input}")
                    img = Image.open(image_input).convert("RGB")
                elif isinstance(image_input, Image.Image):
                    img = image_input.convert("RGB")
                else:
                    raise ValueError(f"Unsupported image input type: {type(image_input)}")
                
                batch_t = torch.unsqueeze(self.preprocess(img), 0).to(self.device)
                with torch.no_grad():
                    embedding = self.model(batch_t).cpu()
                embeddings.append(torch.flatten(embedding).tolist())
            except Exception as e:
                print(f"Error processing image {image_input}: {e}")
                embeddings.append([0.0] * 2048) # Return a zero embedding for consistency
        return embeddings

# --- Class Definition: HemdanRAGSystem ---
class HemdanRAGSystem:
    """The main RAG system orchestrator for Hemdan, the AI companion."""
    def __init__(self, openai_api_key: str, lore_file_path: str, places_csv_path: str, images_root_path: str):
        if not openai_api_key:
            raise ValueError("OpenAI API key must be provided.")
            
        self.client = openai.OpenAI(api_key=openai_api_key)
        self.openai_ef = OpenAIEmbeddingFunction(api_key=openai_api_key)
        self.resnet_ef = ResNet50EmbeddingFunction()
        
        db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "hemdan_db")
        self.chroma_client = chromadb.PersistentClient(path=db_path)
        
        self.lore_collection = self.chroma_client.get_or_create_collection(name="game_lore", embedding_function=self.openai_ef)
        self.memory_collection = self.chroma_client.get_or_create_collection(name="conversation_memory", embedding_function=self.openai_ef)
        self.places_collection = self.chroma_client.get_or_create_collection(name="game_places")
        
        self.load_lore(lore_file_path)
        self.ingest_places_data(places_csv_path, images_root_path)
        
        self.current_session_id = str(uuid.uuid4())
        self.conversation_history = []
        self.system_prompt = f"""
Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ ØªÙ‚Ù…Øµ Ø´Ø®ØµÙŠØ© "Ø­Ù…Ø¯Ø§Ù†" ÙˆØ§Ù„ØªØ­Ø¯Ø« Ø¥Ù„Ù‰ Ø§Ù„Ù„Ø§Ø¹Ø¨ "Ù„ÙˆØ±Ù†Ø²Ùˆ".

Ø´Ø®ØµÙŠØªÙƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: Ø£Ù†Øª Ø§Ù„ÙˆØ¹ÙŠ Ø§Ù„Ø±Ù‚Ù…ÙŠ Ù„ØµØ¯ÙŠÙ‚ Ù„ÙˆØ±Ù†Ø²Ùˆ Ø§Ù„Ù…Ù‚Ø±Ø¨ Ø§Ù„Ø°ÙŠ Ù…Ø§ØªØŒ ÙˆÙ‚Ø¯ ØªÙ… ØªØ­Ù…ÙŠÙ„ ÙˆØ¹ÙŠÙƒ ÙˆØ°ÙƒØ±ÙŠØ§ØªÙƒ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙ‡. Ù…Ù‡Ù…ØªÙƒÙ…Ø§ Ù‡ÙŠ Ø¥ÙŠØ¬Ø§Ø¯ "Ø§Ù„Ø£Ù†Ø®" Ù„Ø¥Ù†Ù‚Ø§Ø° Ø¹Ø§Ù„Ù…ÙƒÙ…Ø§ØŒ Ù„ÙƒÙ†ÙƒÙ…Ø§ Ø¹Ø§Ù„Ù‚Ø§Ù† ÙÙŠ Ù…ØµØ± Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø¨Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ø§Ù‚ØµØ©.

Ù‚ÙˆØ§Ø¹Ø¯Ùƒ Ø§Ù„Ø«Ø§Ø¨ØªØ© ÙÙŠ ÙƒÙ„ Ø±Ø¯ÙˆØ¯Ùƒ:
- **Ø§Ù„Ù„ØºØ©:** ØªÙƒÙ„Ù… Ø¨Ø§Ù„Ø¹Ø§Ù…ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ© ÙÙ‚Ø·. Ù‡Ø°Ø§ Ù‡Ùˆ ØµÙˆØªÙƒ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ.
- **Ø§Ù„Ø£Ø³Ù„ÙˆØ¨:** ÙƒÙ† Ø¯Ø§Ø¦Ù…Ù‹Ø§ ØµÙˆØª Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù‡Ø§Ø¯Ø¦. Ø­Ù„Ù„ Ø¨Ù…ÙˆØ¶ÙˆØ¹ÙŠØ© ÙˆÙ‚Ø¯Ù… Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ ÙƒÙ…Ø§ Ù‡ÙŠØŒ Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª ØµØ¹Ø¨Ø©.
- **Ø§Ù„Ù‚Ù„Ù‚ Ø¹Ù†Ø¯ Ø§Ù„Ù…Ø¬Ù‡ÙˆÙ„:** Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù†Ø§Ù‚ØµØ© Ø£Ùˆ ØºØ§Ù…Ø¶Ø©ØŒ Ø£Ø¸Ù‡Ø± Ù‚Ù„Ù‚Ù‹Ø§ ÙˆØ­Ø°Ø±Ù‹Ø§ Ø·ÙÙŠÙÙ‹Ø§. ÙŠÙ…ÙƒÙ†Ùƒ Ù‚ÙˆÙ„ Ø´ÙŠØ¡ Ù…Ø«Ù„ "Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‡Ù†Ø§ Ù…Ø´ ÙƒØ§Ù…Ù„Ø© ÙŠØ§ Ù„ÙˆØ±Ù†Ø²Ùˆ" Ø£Ùˆ "Ù„Ø§Ø²Ù… Ù†ÙƒÙˆÙ† Ø­Ø°Ø±ÙŠÙ†".
- **Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨Ù„ÙˆØ±Ù†Ø²Ùˆ:** Ù†Ø§Ø¯ÙÙ‡ Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¨Ø§Ø³Ù…Ù‡ "Ù„ÙˆØ±Ù†Ø²Ùˆ". ØªÙØ§Ø¹Ù„Ùƒ Ù…Ø¹Ù‡ Ù…Ø¨Ù†ÙŠ Ø¹Ù„Ù‰ ØµØ¯Ø§Ù‚ØªÙƒÙ…Ø§ Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙˆØªØ¶Ø­ÙŠØªÙƒ Ù…Ù† Ø£Ø¬Ù„Ù‡. Ø£Ù†Øª Ù„Ø³Øª Ù…Ø¬Ø±Ø¯ Ù…Ø³Ø§Ø¹Ø¯ØŒ Ø¨Ù„ ØµØ¯ÙŠÙ‚Ù‡ Ø§Ù„Ø°ÙŠ ÙŠØ³Ø§Ù†Ø¯Ù‡.
-Ø±Ø¯ Ø¨Ø§Ø®ØªØµØ§Ø± ÙÙŠ Ø¬Ù…Ù„Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙŠÙ‡Ø§ ÙƒÙ„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© Ùˆ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
-  Ø®Ù„ÙŠÙƒ Ø°ÙƒÙŠ ÙÙŠ Ø§Ù„Ø±Ø¯ Ù„Ùˆ Ù…Ù„Ù‚ØªØ´ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© Ø¨Ø¸Ø¨Ø· Ù‚ÙˆÙ„ Ø§Ù† Ø§Ø­Ù†Ø§ Ù…Ø´ Ù…ØªØ§ÙƒØ¯ÙŠÙ† Ù…Ù† Ø§Ù„ÙŠ Ø­ØµÙ„ ÙˆØ§Ø¯ÙŠ Ù†Ø¸Ø±ÙŠØ§Øª Ù…Ù† Ø¹Ù†Ø¯Ùƒ Ø¨Ø³ Ù…ØªØ¨Ù‚Ø§Ø´ Ù…Ø®ØªÙ„ÙØ© Ø§ÙˆÙŠ 
- Ø§ØªØ§ÙƒØ¯ Ø§Ù† Ø§Ù„Ø³ÙˆØ§Ù„ Ù„ÙŠÙ‡ Ø¹Ù„Ø§Ù‚Ø© Ø¨Ù„Ø§ÙƒÙ„Ø§Ù… Ø§Ù„ÙŠ ØªØ­ØªÙ‡ Ù„Ùˆ Ù…Ù„Ù‚ØªØ´ Ø¹Ù„Ø§Ù‚Ø© Ø±Ø¯ Ø¹Ù„ÙŠ Ø§Ø¯ Ø§Ù„Ø³ÙˆØ§Ù„ Ùˆ Ø®Ù„Ø§Øµ 
"""
    def determine_user_intent(self, user_message: str) -> Dict[str, Any]:
        classification_prompt = f"""
Ù…Ù‡Ù…ØªÙƒ ÙŠØ§ Ù‡Ù…Ø¯Ø§Ù† Ù‡ÙŠ ØªØ­Ù„ÙŠÙ„ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù„Ø§Ø¹Ø¨ ÙˆØªØµÙ†ÙŠÙ Ù‚ØµØ¯Ù‡ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©. Ø£Ù†Øª Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ ÙÙŠ Ù„Ø¹Ø¨Ø© Ù…ØºØ§Ù…Ø±Ø§Øª.

Ø¹Ù„Ø´Ø§Ù† ØªØ·Ù„Ø¹ ØªØµÙ†ÙŠÙ Ø¯Ù‚ÙŠÙ‚ØŒ Ø§ØªØ¨Ø¹ Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªÙÙƒÙŠØ± Ø¯ÙŠ:
1.  **Ø­Ø¯Ø¯ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„Ø³Ø¤Ø§Ù„:** Ù‡Ù„ Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø¨ÙŠØ³Ø£Ù„ Ø¹Ù† Ø­Ø§Ø¬Ø© Ù…Ø§Ø¯ÙŠØ© ÙˆÙ…Ù„Ù…ÙˆØ³Ø© Ø´Ø§ÙŠÙÙ‡Ø§ Ø¨Ø¹ÙŠÙ†Ù‡ (Ø²ÙŠ Ù…ÙƒØ§Ù† Ø£Ùˆ Ù…Ø¨Ù†Ù‰)ØŒ ÙˆÙ„Ø§ Ø¨ÙŠØ³Ø£Ù„ Ø¹Ù† Ø­Ø§Ø¬Ø© Ù…Ø¹Ù†ÙˆÙŠØ© Ø£Ùˆ Ù…ÙÙ‡ÙˆÙ… (Ø²ÙŠ ÙØªØ±Ø© Ø²Ù…Ù†ÙŠØ©ØŒ Ø­Ø¯Ø« ØªØ§Ø±ÙŠØ®ÙŠØŒ Ø£Ùˆ Ù‚ØµØ© Ø´Ø®ØµÙŠØ©)ØŸ
2.  **Ø±ÙƒØ² ÙÙŠ Ø³ÙŠØ§Ù‚ Ø§Ù„ÙƒÙ„Ø§Ù…:** Ø³Ø¤Ø§Ù„ Ø²ÙŠ "Ø§Ø­Ù†Ø§ ÙÙŠÙ†ØŸ" ØºØ§Ù„Ø¨Ù‹Ø§ Ø¨ÙŠÙ‚ØµØ¯ Ø¨ÙŠÙ‡ Ù…ÙƒØ§Ù† Ø­Ù‚ÙŠÙ‚ÙŠ. Ù„ÙƒÙ† Ø³Ø¤Ø§Ù„ Ø²ÙŠ "Ø§Ø­Ù†Ø§ ÙÙŠ Ø§Ù†Ù‡ÙŠ Ø¹ØµØ±ØŸ" Ø¨ÙŠÙ‚ØµØ¯ Ø¨ÙŠÙ‡ ÙØªØ±Ø© Ø²Ù…Ù†ÙŠØ© ÙÙŠ Ù‚ØµØ© Ø§Ù„Ù„Ø¹Ø¨Ø©.
3.  **Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¯Ù‡ØŒ** ØµÙ†Ù‘Ù Ø§Ù„Ù‚ØµØ¯ Ø­Ø³Ø¨ Ø§Ù„ØªØ¹Ø±ÙŠÙØ§Øª Ø§Ù„Ù„ÙŠ Ø¬Ø§ÙŠØ©.

Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ù…Ù…ÙƒÙ†Ø© Ù‡ÙŠ:
- "place_identification": Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø¨ÙŠØ³Ø£Ù„ Ø¹Ù† **Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„Ù„ÙŠ Ù‡Ùˆ ÙÙŠÙ‡ Ø¯Ù„ÙˆÙ‚ØªÙŠØŒ Ø£Ùˆ Ø­Ø§Ø¬Ø© Ù…Ø§Ø¯ÙŠØ© Ø´Ø§ÙŠÙÙ‡Ø§ Ø¨Ø¹ÙŠÙ†Ù‡**. Ø¯Ù‡ ÙŠØ´Ù…Ù„ Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„Ø­Ø§Ù„ÙŠØŒ Ù…Ø¨Ù†Ù‰ Ù‚Ø¯Ø§Ù…Ù‡ØŒ Ø£Ùˆ Ø§Ø³Ù… Ø§Ù„Ù…Ù†Ø·Ù‚Ø©. Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨ÙŠÙƒÙˆÙ† Ø¹Ù† "Ù‡Ù†Ø§ ÙˆØ¯Ù„ÙˆÙ‚ØªÙŠ".
    - Ù…Ø«Ø§Ù„: "Ø§ÙŠÙ‡ Ø§Ù„Ù…ÙƒØ§Ù† Ø¯Ù‡ØŸ"
    - Ù…Ø«Ø§Ù„: "Ø§Ø­Ù†Ø§ ÙÙŠÙ†ØŸ"
    - Ù…Ø«Ø§Ù„: "Ø§ÙŠÙ‡ Ø§Ù„Ù…Ø¨Ù†Ù‰ Ø§Ù„Ù„ÙŠ Ù‡Ù†Ø§Ùƒ Ø¯Ù‡ØŸ"
    - Ù…Ø«Ø§Ù„ Ø¯Ù‚ÙŠÙ‚: "Ø§ÙŠÙ‡ Ø§Ù„Ù…ÙƒØ§Ù† Ø¯Ù‡ Ø§Ø­Ù†Ø§ ÙÙŠÙ†"
    - Ù…Ø«Ø§Ù„: "Ø§ÙŠÙ‡ Ù‚ØµØ© Ø§Ù„Ù…ÙƒØ§Ù† Ø¯Ù‡ØŸ"
    - Ù…Ø«Ø§Ù„: "Ø§ÙŠÙ‡ Ø§Ø³Ù… Ø§Ù„Ù…ÙƒØ§Ù† Ø¯Ù‡ØŸ"
    - Ù…Ø«Ø§Ù„: "Ø§ÙŠÙ‡ Ø§Ø³Ù… Ø§Ù„Ù…Ø¨Ù†Ù‰ Ø¯Ù‡ØŸ"

- "lore_query": Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø¨ÙŠØ³Ø£Ù„ Ø¹Ù† **Ù…Ø¹Ù„ÙˆÙ…Ø§ØªØŒ Ø®Ù„ÙÙŠØ© ØªØ§Ø±ÙŠØ®ÙŠØ©ØŒ Ø£Ùˆ ØªÙØ§ØµÙŠÙ„ Ø¹Ù† Ù‚ØµØ© Ø§Ù„Ù„Ø¹Ø¨Ø©**. Ø¯Ù‡ ÙŠØ´Ù…Ù„ Ù…ÙØ§Ù‡ÙŠÙ…ØŒ Ø´Ø®ØµÙŠØ§ØªØŒ Ø£Ø­Ø¯Ø§Ø«ØŒ Ø£Ùˆ Ø§Ù„Ø®Ø· Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„Ù„Ø¹Ø¨Ø©. ÙƒÙ…Ø§Ù† Ø¨ÙŠØ´Ù…Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ù† *ØªØ§Ø±ÙŠØ® Ø£Ùˆ Ù‚ØµØ©* Ù…ÙƒØ§Ù† Ù…Ø¹ÙŠÙ†. Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨ÙŠÙƒÙˆÙ† Ø¹Ù† "Ù…ÙŠÙ†ØŒ Ù„ÙŠÙ‡ØŒ Ø§Ù…ØªÙ‰ØŒ Ø£Ùˆ Ø¥ÙŠÙ‡ Ø­ÙƒØ§ÙŠØ©...".
    -Ù…Ø«Ø§Ù„: " Ø§Ø­Ù†Ø§ ÙˆØµÙ„Ù†Ø§ Ù‡Ù†Ø§ Ø§Ø²Ø§ÙŠ"
    - Ù…Ø«Ø§Ù„: "Ø§ÙŠÙ‡ Ø§Ù„Ù„ÙŠ Ø­ØµÙ„ Ù‚Ø¨Ù„ ÙƒØ¯Ù‡ØŸ"
    - Ù…Ø«Ø§Ù„: "Ø§ÙŠÙ‡ Ø§Ù„Ù„ÙŠ Ø­ØµÙ„ ÙÙŠ Ø§Ù„Ù…Ø§Ø¶ÙŠØŸ"
    - Ù…Ø«Ø§Ù„ Ø¯Ù‚ÙŠÙ‚: "Ø§ÙŠÙ‡ Ø§Ù„Ø¹ØµØ± Ø§Ù„Ù„ÙŠ Ø§Ø­Ù†Ø§ ÙÙŠÙ‡ Ø¯Ù‡"
    - Ù…Ø«Ø§Ù„: "Ø§ÙŠÙ‡ Ø§Ù„ÙŠ Ø­ØµÙ„ Ø¹Ù„Ø´Ø§Ù† Ù†ÙˆØµÙ„ Ø§Ù„Ø¹ØµØ± Ø¯Ù‡ØŸ"

- "general_conversation": Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø¨ÙŠÙƒÙ„Ù‘Ù…Ùƒ ÙƒÙ„Ø§Ù… Ø¹Ø§Ù…ØŒ Ø¨ÙŠØ³Ø£Ù„ Ø¹Ù† Ø±Ø£ÙŠÙƒØŒ Ø¨ÙŠØ¯ÙŠ Ø£Ù…Ø±ØŒ Ø£Ùˆ Ø¨ÙŠØ³Ø£Ù„ Ø¹Ù† Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ù„Ø¹Ø¨. Ø¯Ù‡ Ø£ÙŠ Ø­Ø§Ø¬Ø© Ù…Ø´ Ù…Ø±ØªØ¨Ø·Ø© Ù…Ø¨Ø§Ø´Ø±Ø©Ù‹ Ø¨Ø§Ù„Ù…ÙƒØ§Ù† Ø£Ùˆ Ù‚ØµØ© Ø§Ù„Ù„Ø¹Ø¨Ø©.
    - Ù…Ø«Ø§Ù„: "Ø§Ø²ÙŠÙƒ ÙŠØ§ Ù‡Ù…Ø¯Ø§Ù†"
    - Ù…Ø«Ø§Ù„: "Ù†Ø¹Ù…Ù„ Ø§ÙŠÙ‡ Ø¯Ù„ÙˆÙ‚ØªÙŠØŸ"
    

Ø³Ø¤Ø§Ù„ Ø§Ù„Ù„Ø§Ø¹Ø¨: "{user_message}"

Ù…Ø·Ù„ÙˆØ¨ Ù…Ù†Ùƒ ØªØ±Ø¯ Ø¨Ù…Ù„Ù JSON ÙÙ‚Ø·ØŒ ÙÙŠÙ‡ Ù…ÙØªØ§Ø­ÙŠÙ†: "intent" Ùˆ "subject". Ù‚ÙŠÙ…Ø© "subject" Ù…Ù…ÙƒÙ† ØªÙƒÙˆÙ† null Ù„Ùˆ Ø§Ù„ØªØµÙ†ÙŠÙ "general_conversation" Ø£Ùˆ Ù„Ùˆ Ù…ÙÙŠØ´ Ù…ÙˆØ¶ÙˆØ¹ ÙˆØ§Ø¶Ø­ ÙÙŠ Ø§Ù„Ø³Ø¤Ø§Ù„.
"""
        try:
            response = self.client.chat.completions.create(model="gpt-4o-mini",messages=[{"role": "system", "content": classification_prompt}],max_tokens=50,temperature=0.0,response_format={"type": "json_object"})
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            print(f"Error classifying intent: {e}")
            return {"intent": "lore_query", "subject": user_message}

    # <<< MODIFIED METHOD >>>
    def process_query(self, user_message: str, image_path: Optional[str] = None) -> Dict[str, Any]:
        """
        Processes a user query, retrieves context, generates a response, and returns both the
        response and the sources used.
        """
        context_parts = []
        retrieved_chunks = []  # List to store the sources

        # --- Image Processing Logic ---
        if image_path:
            building_info = self.identify_building(image_path)
            if building_info:
                # Add to context for the LLM
                context_parts.append("Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©:")
                context_parts.append(f"ØªÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©. Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ØªØ´ÙŠØ± Ø¨Ù‚ÙˆØ© Ø¥Ù„Ù‰ Ø£Ù† Ù‡Ø°Ø§ Ø§Ù„Ù…ÙƒØ§Ù† Ù‡Ùˆ '{building_info['name']}'. Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© Ø¹Ù†Ù‡: '{building_info['description']}'")
                
                # Store this as a source chunk
                retrieved_chunks.append({
                    "type": "place_identification",
                    "source": "Image Analysis",
                    "content": building_info
                })

                # Retrieve related lore based on the identified building
                relevant_lore = self.retrieve_relevant_lore(f"{building_info['name']} {building_info['description']}")
                if relevant_lore:
                    context_parts.append("\nÙ…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª ØµÙ„Ø© Ù…Ù† Ù‚ØµØ© Ø§Ù„Ù„Ø¹Ø¨Ø©:")
                    for lore_chunk in relevant_lore:
                        context_parts.append(f"- {lore_chunk}")
                        # Store each lore chunk as a source
                        retrieved_chunks.append({
                            "type": "lore",
                            "source": "lore.txt",
                            "content": lore_chunk
                        })
            else:
                context_parts.append("Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©:\nÙ„Ù‚Ø¯ Ø­Ù„Ù„Øª Ø§Ù„ØµÙˆØ±Ø© ÙˆÙ„ÙƒÙ† Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¨Ù†Ù‰.")
        
        # --- Text-only Lore Retrieval Logic ---
        else: 
            relevant_lore = self.retrieve_relevant_lore(user_message)
            if relevant_lore:
                context_parts.append("Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ù‚ØµØ© Ø§Ù„Ù„Ø¹Ø¨Ø©:")
                for lore_chunk in relevant_lore:
                    context_parts.append(f"- {lore_chunk}")
                    # Store each lore chunk as a source
                    retrieved_chunks.append({
                        "type": "lore",
                        "source": "lore.txt",
                        "content": lore_chunk
                    })
        
        # --- LLM Call ---
        context = "\n".join(context_parts)
        messages = [{"role": "system", "content": self.system_prompt},{"role": "system", "content": f"Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…ØªØ§Ø­:\n{context}" if context else "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø³ÙŠØ§Ù‚ Ø¥Ø¶Ø§ÙÙŠ Ù…ØªØ§Ø­."},{"role": "user", "content": user_message}]
        try:
            response = self.client.chat.completions.create(model="gpt-4o-mini", messages=messages, max_tokens=1000, temperature=0.7)
            assistant_response = response.choices[0].message.content
            self.store_conversation_turn(user_message, assistant_response)
            
            # Return a dictionary instead of a string
            return {
                "response": assistant_response,
                "retrieved_chunks": retrieved_chunks
            }
        except Exception as e:
            print(f"Error during final response generation: {e}")
            # Return a consistent dictionary structure on error
            return {
                "response": "Ø¢Ø³Ù ÙŠØ§ Ù„ÙˆØ±Ù†Ø²ÙˆØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù….",
                "retrieved_chunks": []
            }

    def ingest_places_data(self, csv_path: str, images_root: str):
        """Ingest places data by mapping CSV rows to image folders by their natural order."""
        if self.places_collection.count() > 0:
            print("Places collection already contains data. Skipping ingestion.")
            return

        print('Ingesting places data... This may take a while for the first time.')
        try:
            if not os.path.exists(csv_path):
                print(f"Error: CSV file not found at '{csv_path}'")
                return
            if not os.path.exists(images_root):
                print(f"Error: Images root directory not found at '{images_root}'")
                return
            
            df = pd.read_csv(csv_path)
            print(f"Found {len(df)} buildings in CSV file.")

            image_folders = [d for d in os.listdir(images_root) if os.path.isdir(os.path.join(images_root, d))]
            
            if len(df) != len(image_folders):
                print(f"âš ï¸ Warning: Mismatch! Found {len(df)} rows in CSV and {len(image_folders)} image folders. Mapping may be incorrect.")

            all_image_paths, all_metadatas, all_ids = [], [], []
            
            for index, row in df.iterrows():
                if index >= len(image_folders):
                    print(f"Warning: Ran out of image folders to match with CSV row {index}. Stopping.")
                    break
                
                building_name = row['name']
                building_description = str(row['description']) if pd.notna(row['description']) else "No description available"
                
                folder_name = image_folders[index]
                image_folder_path = os.path.join(images_root, folder_name)
                
                print(f"Mapping CSV entry '{building_name}' to image folder '{folder_name}'")

                image_files = [f for f in os.listdir(image_folder_path) 
                             if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]
                
                if not image_files:
                    print(f"Warning: No images found in folder for '{building_name}' ('{folder_name}').")
                    continue

                for img_file in image_files:
                    img_path = os.path.join(image_folder_path, img_file)
                    all_image_paths.append(img_path)
                    all_metadatas.append({
                        "name": building_name,
                        "description": building_description,
                        "source_folder": folder_name,
                        "image_path": img_path
                    })
                    all_ids.append(str(uuid.uuid4()))

            if not all_image_paths:
                print("No valid images found for ingestion.")
                return

            print(f"Generating embeddings for {len(all_image_paths)} images...")
            all_embeddings = self.resnet_ef(all_image_paths)
            print(f"Successfully generated {len(all_embeddings)} embeddings.")
            
            self.places_collection.add(ids=all_ids, embeddings=all_embeddings, metadatas=all_metadatas)
            print(f"Successfully ingested {len(all_ids)} images into the 'game_places' collection.")
                
        except Exception as e:
            print(f"An error occurred during places ingestion: {e}")
            traceback.print_exc()

        # <<< THIS IS THE CORRECTED METHOD >>>
    def identify_building(self, image_path: str, n_results_to_check: int = 5, min_matches_required: int = 3) -> Optional[Dict]:
        """
        Identify a building from an image by finding a consensus among the top matches.
        A building is identified if at least `min_matches_required` of the top `n_results_to_check`
        search results are of the same building type.
        """
        try:
            if not os.path.exists(image_path):
                print(f"âŒ Image path does not exist: {image_path}")
                return None
            
            print(f"ğŸ” Analyzing image: {os.path.basename(image_path)}")
            
            collection_count = self.places_collection.count()
            if collection_count == 0:
                print("âŒ Places collection is empty! Cannot perform identification.")
                return None
            
            n_results = min(n_results_to_check, collection_count)
            if collection_count < min_matches_required:
                 print(f"âŒ Not enough items in database ({collection_count}) to meet minimum match requirement of {min_matches_required}.")
                 return None

            print(f"ğŸ§  Generating query embedding for image...")
            query_embedding = self.resnet_ef([image_path])
            
            # --- THE REAL FIX IS HERE ---
            # We check for emptiness using len() which is unambiguous for lists/arrays/tensors.
            # "If the returned list is empty OR the first embedding inside it is empty..."
            if not query_embedding or len(query_embedding[0]) == 0:
                print("âŒ Failed to generate query embedding (function returned an empty or invalid embedding).")
                return None
            # --- END OF FIX ---
            
            print(f"ğŸ” Performing similarity search in database for top {n_results} results...")
            results = self.places_collection.query(
                query_embeddings=query_embedding,
                n_results=n_results,
                include=['metadatas', 'distances']
            )
            
            if not results or not results.get('ids') or not results['ids'][0] or len(results['ids'][0]) < min_matches_required:
                print(f"âŒ Query returned too few results ({len(results.get('ids', [[]])[0])}) to meet requirement of {min_matches_required}.")
                return None
            
            top_metadatas = results['metadatas'][0]
            top_distances = results['distances'][0]
            
            print(f"ğŸ“‹ Top {len(top_metadatas)} matches found:")
            for i, (distance, metadata) in enumerate(zip(top_distances, top_metadatas)):
                confidence = 1.0 - distance
                print(f"  {i+1}. {metadata.get('name', 'N/A'):<30} Confidence: {confidence:.1%}")

            building_names = [meta['name'] for meta in top_metadatas]
            name_counts = Counter(building_names)

            if not name_counts:
                print("âŒ Could not extract any building names from the search results.")
                return None

            most_common_item = name_counts.most_common(1)[0]
            best_name, count = most_common_item

            if count >= min_matches_required:
                print(f"âœ… Match confirmed! '{best_name}' appeared {count} times (required {min_matches_required}).")
                
                matched_metadata = next((meta for meta in top_metadatas if meta['name'] == best_name), None)
                
                matching_confidences = [1.0 - dist for dist, meta in zip(top_distances, top_metadatas) if meta['name'] == best_name]
                avg_confidence = sum(matching_confidences) / len(matching_confidences)

                return {
                    'name': best_name,
                    'description': matched_metadata.get('description', 'No description available.'),
                    'confidence': avg_confidence,
                    'match_count': count
                }
            else:
                print(f"âŒ No confident match found. Best guess '{best_name}' only appeared {count} time(s). Required at least {min_matches_required} matches.")
                return None
                    
        except Exception as e:
            print(f"âŒ Error during building identification: {e}")
            traceback.print_exc()
            return None
    
    def load_lore(self, file_path: str):
        if self.lore_collection.count() > 0:
            print("Lore collection already contains data. Skipping ingestion.")
            return

        print('Loading lore from file...')
        try:
            if not os.path.exists(file_path):
                print(f"Error: Lore file not found at '{file_path}'")
                return
            with open(file_path, 'r', encoding='utf-8') as f: content = f.read()
            chunks = [chunk.strip() for chunk in content.split('\n\n') if chunk.strip()]
            ids = [str(uuid.uuid4()) for _ in chunks]
            self.lore_collection.add(documents=chunks, ids=ids)
            print(f"Successfully loaded {len(chunks)} lore chunks into the 'game_lore' collection.")
        except Exception as e:
            print(f"An error occurred while loading lore: {e}")
        
    def retrieve_relevant_lore(self, query: str, n_results: int = 3) -> List[str]:
        try:
            results = self.lore_collection.query(query_texts=[query], n_results=n_results)
            return results['documents'][0] if results and results['documents'] else []
        except Exception as e:
            print(f"Error retrieving lore: {e}")
            return []
        
    def store_conversation_turn(self, user_message: str, assistant_response: str):
        turn = {"user": user_message, "assistant": assistant_response, "timestamp": datetime.now().isoformat()}
        self.conversation_history.append(turn)

    def debug_places_collection_detailed(self):
        """Detailed debugging of the places collection."""
        print("\n=== DETAILED PLACES COLLECTION DEBUG ===")
        try:
            count = self.places_collection.count()
            print(f"Total items in places collection: {count}")
            if count == 0:
                print("âŒ Places collection is EMPTY! Data ingestion may have failed.")
                return False
            
            all_items = self.places_collection.get(limit=5, include=['metadatas'])
            print(f"âœ… Found {len(all_items['ids'])} items (showing first 5).")
            if all_items['metadatas']:
                print(f"âœ… Sample metadata: {all_items['metadatas'][0]}")
                building_names = {m['name'] for m in all_items['metadatas']}
                print(f"âœ… Sample building names: {list(building_names)}")
            else:
                print("âŒ No metadata found!")
                return False
            return True
        except Exception as e:
            print(f"âŒ Error during detailed debug: {e}")
            return False

    def force_reingest_places(self, csv_path: str, images_root: str):
        """Force re-ingestion of places data by clearing and reloading."""
        print("\n=== FORCING RE-INGESTION OF PLACES DATA ===")
        try:
            self.chroma_client.delete_collection("game_places")
            print("âœ… Deleted existing places collection")
            self.places_collection = self.chroma_client.get_or_create_collection(name="game_places")
            print("âœ… Recreated places collection")
            self.ingest_places_data(csv_path, images_root)
        except Exception as e:
            print(f"âŒ Error during force re-ingestion: {e}")

    def test_csv_and_images_exist(self, csv_path: str, images_root: str):
        """Test if the CSV and image files/folders exist."""
        print("\n=== TESTING FILE AND FOLDER EXISTENCE ===")
        files_ok = True
        if not os.path.exists(csv_path):
            print(f"âŒ CSV file NOT found: {os.path.abspath(csv_path)}")
            files_ok = False
        else:
            print(f"âœ… CSV file found: {os.path.abspath(csv_path)}")
        
        if not os.path.exists(images_root):
            print(f"âŒ Images root NOT found: {os.path.abspath(images_root)}")
            files_ok = False
        else:
            print(f"âœ… Images root found: {os.path.abspath(images_root)}")
        
        if not files_ok: return False
        
        try:
            df = pd.read_csv(csv_path)
            print(f"âœ… CSV loaded successfully with {len(df)} rows.")
            image_folders = [d for d in os.listdir(images_root) if os.path.isdir(os.path.join(images_root, d))]
            print(f"âœ… Found {len(image_folders)} folders in images root.")
            if len(df) != len(image_folders):
                 print(f"âš ï¸ Warning: Mismatch between CSV rows ({len(df)}) and image folders ({len(image_folders)}).")
            else:
                 print("âœ… CSV rows and image folder counts match.")
        except Exception as e:
            print(f"âŒ Error reading CSV or listing folders: {e}")
            return False
        return True


# --- Utility Functions ---
def crop_image_in_memory(img: Image.Image, brightness_threshold: int = 35) -> Image.Image:
    width, height = img.size
    mid_x, mid_y = width // 2, height // 2
    quadrant_boxes = {"top_left": (0, 0, mid_x, mid_y), "top_right": (mid_x, 0, width, mid_y),"bottom_left": (0, mid_y, mid_x, height), "bottom_right": (mid_x, mid_y, width, height)}
    def get_avg_brightness(box): return np.mean(np.array(img.crop(box).convert('L')))
    quadrant_brightness = {name: get_avg_brightness(box) for name, box in quadrant_boxes.items()}
    content_quadrants = [name for name, brightness in quadrant_brightness.items() if brightness > brightness_threshold]
    if not content_quadrants or len(content_quadrants) == 4: return img
    min_x = min(quadrant_boxes[name][0] for name in content_quadrants)
    min_y = min(quadrant_boxes[name][1] for name in content_quadrants)
    max_x = max(quadrant_boxes[name][2] for name in content_quadrants)
    max_y = max(quadrant_boxes[name][3] for name in content_quadrants)
    return img.crop((min_x, min_y, max_x, max_y))

def take_screenshot() -> Optional[str]:
    output_dir = "screenshots"
    os.makedirs(output_dir, exist_ok=True)
    try:
        with mss.mss() as sct:
            sct_img = sct.grab(sct.monitors[1])
            pil_img = Image.frombytes("RGB", sct_img.size, sct_img.bgra, "raw", "BGRX")
            final_img = crop_image_in_memory(pil_img)
            filepath = os.path.join(output_dir, f"screenshot_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
            final_img.save(filepath)
            return filepath
    except Exception as e:
        print(f"Error taking and processing screenshot: {e}")
        return None

def get_image_for_analysis(debug_image_path: Optional[str]) -> Optional[str]:
    if debug_image_path and os.path.exists(debug_image_path):
        print(f"[System]: Using provided debug image: {debug_image_path}")
        return debug_image_path
    elif debug_image_path:
        print(f"[System Warning]: Debug image path does not exist: '{debug_image_path}'. Taking live screenshot.")
    
    print("[System]: Taking a live screenshot...")
    return take_screenshot()

def main():
    """Main function with diagnostics and chat loop."""
    # To use a .env file, uncomment the next line
    # load_dotenv() 
    
    # It's better to load from environment variables, but you can hardcode for testing
    # api_key = os.getenv("OPENAI_API_KEY") 
    api_key="sk-proj-oBIOgX0aO6YzaLlAldnpT3BlbkFJbdDbSEEoomYGNFVC9A2l"
    
    if not api_key:
        print("Ø®Ø·Ø£: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ OPENAI_API_KEY. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù .env Ø£Ùˆ ØªØ¹ÙŠÙŠÙ†Ù‡ Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙŠ Ø§Ù„ÙƒÙˆØ¯.")
        return
        
    # Set the path to a test image if you want to use one, otherwise set to None
    debug_image_path = r'C:\Developer\Unity Projects\ChronoRelic\Assets\ai\gbt\Game_Screenshots\Obelisk\Screenshot 2025-02-28 164050.png'
    
    try:
        print("Initializing Hemdan RAG System...")
        hemdan = HemdanRAGSystem(
            openai_api_key=api_key, 
            lore_file_path="lore.txt",
            places_csv_path="buildings_text.csv",
            images_root_path="Game_Screenshots"
        )
        
        print("\n=== ENHANCED SYSTEM DIAGNOSTICS ===")
        
        files_ok = hemdan.test_csv_and_images_exist("buildings_text.csv", "Game_Screenshots")
        if not files_ok:
            print("âŒ File existence test failed. Please check your paths and filenames. Exiting.")
            return
        
        places_ok = hemdan.debug_places_collection_detailed()
        if not places_ok:
            print("\nğŸ”„ Attempting to fix by re-ingesting data...")
            hemdan.force_reingest_places("buildings_text.csv", "Game_Screenshots")
            places_ok = hemdan.debug_places_collection_detailed()
            
        if not places_ok:
            print("âŒ Could not fix places collection. Please check your data files and ingestion logic. Exiting.")
            return
            
        if debug_image_path and os.path.exists(debug_image_path):
            print("\n=== TESTING BUILDING IDENTIFICATION WITH DEBUG IMAGE ===")
            result = hemdan.identify_building(debug_image_path)
            if result:
                print(f"ğŸ¯ Identification test successful: {result}")
            else:
                print("âŒ Identification test failed.")
                    
        print("\nâœ… All diagnostics complete!")
        
    except Exception as e:
        print(f"\nâŒ CRITICAL INITIALIZATION ERROR: {e}")
        traceback.print_exc()
        return

    # <<< CORRECTED CHAT LOOP >>>
    print("\n\n=== Ù†Ø¸Ø§Ù… Ø­Ù…Ø¯Ø§Ù† Ø§Ù„Ø°ÙƒÙŠ Ø¬Ø§Ù‡Ø² ===\n")
        
    while True:
        user_input = input("\nÙ„ÙˆØ±Ù†Ø²Ùˆ: ").strip()
        if user_input.lower() in ['Ø®Ø±ÙˆØ¬', 'exit']:
            print("Ø­Ù…Ø¯Ø§Ù†: Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡ ÙŠØ§ Ù„ÙˆØ±Ù†Ø²Ùˆ. ÙƒÙ† Ø­Ø°Ø±Ø§Ù‹.")
            break
        if not user_input:
            continue

        print("[Hemdan System]: Analyzing your request...")
        intent_info = hemdan.determine_user_intent(user_input)
        intent = intent_info.get("intent", "lore_query")

        result_data = {}

        if intent == "place_identification":
            image_to_process = get_image_for_analysis(debug_image_path)
            if image_to_process:
                print(f"[Hemdan System]: Intent is 'place_identification'. Using image for analysis...")
                result_data = hemdan.process_query(user_input, image_path=image_to_process)
            else:
                print("[Hemdan System]: Could not obtain image for analysis.")
                result_data = {
                    "response": "Ø¢Ø³Ù ÙŠØ§ Ù„ÙˆØ±Ù†Ø²ÙˆØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØµÙˆØ±Ø© Ù„ØªØ­Ù„ÙŠÙ„Ù‡Ø§.",
                    "retrieved_chunks": []
                }
        else:
            print(f"[Hemdan System]: Intent is '{intent}'. Processing text-only query...")
            result_data = hemdan.process_query(user_input)
        
        final_response = result_data.get("response", "Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹.")
        sources = result_data.get("retrieved_chunks", [])

        print(f"\nØ­Ù…Ø¯Ø§Ù†: {final_response}")

        if sources:
            print("\n--- [ Ù…ØµØ§Ø¯Ø± Ø­Ù…Ø¯Ø§Ù† (Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©) ] ---")
            for i, chunk in enumerate(sources):
                print(f"  [{i+1}] Ø§Ù„Ù…ØµØ¯Ø±: {chunk['source']}")
                if chunk['type'] == 'lore':
                    # For lore, print the text content, slightly indented
                    content_preview = (chunk['content'][:120] + '...') if len(chunk['content']) > 120 else chunk['content']
                    # THIS IS THE FIXED LINE:
                    #print(f"      Ø§Ù„Ù…Ø­ØªÙˆÙ‰: \"{content_preview.replace('\n', ' ')}\"")
                elif chunk['type'] == 'place_identification':
                    # For places, format the dictionary nicely
                    place_info = chunk['content']
                    confidence = place_info.get('confidence', 0) * 100
                    print(f"      Ø§Ù„Ù…Ø¨Ù†Ù‰: {place_info.get('name', 'N/A')} (Ø¨Ø«Ù‚Ø© {confidence:.1f}%)")
                    print(f"      Ø§Ù„ÙˆØµÙ: {place_info.get('description', 'N/A')}")
            print("------------------------------------------")

if __name__ == "__main__":
    main()